\chapter{Conclusion and Future Work}

Geo-service portal acts as a underlying framework or foundation for various kind of higher level use cases. Main advantage of this registry service is that we can avail data from various repositories that has been crawled and indexed for others to use. Basic querying facilities can be provided directly on top of registry service. Using geo-spatial crawler we can get the latest geo-spatial data available on the web. Once this data is got from various geo-servers, various kinds of applications can be built on top of that. Building an OGC compliant web service catalog can also be beneficiary as already available software and services can use the registry for various kinds of services with little to no modification of their original code-base.
\newline
\par Geo-service portal can be used for many kind of applications. This applications and use cases include wide variety such as just getting the relevant spatial information for the application to doing in-depth spatial or temporal or spatio-temporal analysis. Other use cases can include finding the spread of the disease on large geographic area. Which of the areas can be affected by flood can also be founded using data from catalog. All this services can work if underlying foundation of catalog service is built.

\section{Contribution of thesis}
Geo-Service portal builds ground work for spatial metadata gathering and publishment. It provides this foundation platform in a modular fashion, such that all of these modules work in a pipeline. Output of one module is feeded as input to other module. This gives independence to update modules irrespective of each other as it has loose coupling.
\newline
\par Spatial web crawler allows to crawl the open web and find existing geo repositories and catalog servers to gather spatial data and store available metadata. This allows better availability of spatial features and data then conventional services.
\newline
\par Spatial catalog server allows us to maintain and publish spatial metadata and provides various OGC compliant services on top of it. This allows not only storage and retrieval of spatial data but also known services and methods attached to retrieval of spatial data and extraction of information from spatial data.
\newline
\par Spatial query orchestration allows retrieval of spatial features in and indexed and ranked manner, which can be useful while working with large number of features. It uses indexing based on quad tree which groups feature based on their proximity from the region of interest. Another advanced method to rank spatial features is by quality preferences, in which quality of feature and it's neighbourhood define the importance of such feature.

\section{Future Scope}
There is significant scope of improvements in the current framework for Geo-service portal as it is designed in a modular way. Extensions and higher level use cases can be provided to any of the modules or newer modules can be added. Some of the advancements to the current features are listed below:
\newline
\par Spatial web crawler module can scrap and crawl other things to make use of other qualities of data the catalog service or repositories that are hosting. This can be useful while ranking the quality of a whole registry with respect to other registries. A score can be provided to crawled registries for the spatial data they contain, size and rarity of the data, by no of times they are referenced and many other such metrics. As spatial data is time driven, catalog service can provide refresh rate, a time after which the catalog service is reloaded to check if their is any change in the data it hosts from the crawled sources. This is useful to find most up to date data from available geo repositories. Analyzing this behavior over a time can also give spatio-temporal behaviour of the data provided by a given repository. For example, spread of city in given range of time and rate of growth can be known. Ranking mechanism can be improved to give scores to repositories and in turn the data crawled from the repositories.
\newpage
\par In today's era, when creating a service reliability, scalability and availability is very crucial things to consider. If the data is hosted on one server and single server architecture is implemented then there might be a high chance of failure on high load. Also the growth of the users and query load on the server cannot be predicted. For this reasons it would be better to consider a cloud based implementation for the above stated architecture. Cloud based implementation for crawler, registry service, and query processor can scale very well in the situations for high load. Cloud based implementation can also be good for availability because on situations of high load, infrastructure can be scaled up to cater the need of high load, in situations of low load the infrastructure can be scaled down to save power and investments. The registry itself can be replicated in the cloud at various geographical places to avail above stated high availability and reliability of the operations. Changes are also be easily and more efficiently done in the cloud because we don't have to shutdown and start the instances again. Cloud instances can be replaced in-place without shutting them down. This ensures versioning can be done of the software and eventual upgradations and roll-out of updates can be provided without hiccups.\\
\par Dynamic deployment of catalog server is the key to achieve high scalability and high availability time. In this approach we can dynamically deploy instances of GeoServer catalog server(s) to accommodate high demand from users. To make it transparent to users, $load\_balancer$ can be used. Similarly we can scale down in case of low demand from users. With these advancements, access control can also be achieved via $load\_balancer$, because it can work as a single point of access for the users. 